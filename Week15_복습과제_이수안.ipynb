{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPbwuv5tTnH3"
      },
      "source": [
        "# **1. 라이브러리 및 기본 설정**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLFHYbdRTnH4",
        "outputId": "8bd8ae06-ef85-4c0f-8e7b-9cf56e1670d2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7c5635c83e10>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvuJkJ5ZTnH5"
      },
      "source": [
        "# **2. 임베딩**\n",
        "- 데이터 임베딩에 대한 간단한 예제"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Z7I01D5wTnH5"
      },
      "outputs": [],
      "source": [
        "## 단어를 인덱스에 매핑하는 딕셔너리\n",
        "word_to_ix = {\"hello\": 0, \"world\": 1}\n",
        "\n",
        "## 임베딩 레이어 생성(2개의 단어, 5차원 임베딩)\n",
        "# nn.Embedding 사용\n",
        "embeds = nn.Embedding(2, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEfi-OCLTnH6",
        "outputId": "59c91800-20c4-438d-d0a3-a29002ce3056"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ],
      "source": [
        "## 임베딩 벡터 생성\n",
        "# 특정 단어에 대한 텐서를 생성하고 임베딩 레이어를 통해 벡터를 조회\n",
        "lookup_tensor = torch.tensor([word_to_ix[\"hello\"]], dtype=torch.long)\n",
        "hello_embed = embeds(lookup_tensor)\n",
        "print(hello_embed) # \"hello\" 단어의 임베딩 벡터 출력"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUqgZH8nTnH6"
      },
      "source": [
        "# **3. 데이터 준비**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vHlSiTSYTnH6"
      },
      "outputs": [],
      "source": [
        "# 컨텍스트 크기/ 임베딩 차원 정의\n",
        "CONTEXT_SIZE = 2\n",
        "EMBEDDING_DIM = 10\n",
        "\n",
        "# Shakespeare 텍스트를 단어 단위로 분리하여 N-Gram 데이터로 저장\n",
        "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
        "And dig deep trenches in thy beauty's field,\n",
        "Thy youth's proud livery so gazed on now,\n",
        "Will be a totter'd weed of small worth held:\n",
        "Then being asked, where all thy beauty lies,\n",
        "Where all the treasure of thy lusty days;\n",
        "To say, within thine own deep sunken eyes,\n",
        "Were an all-eating shame, and thriftless praise.\n",
        "How much more praise deserv'd thy beauty's use,\n",
        "If thou couldst answer 'This fair child of mine\n",
        "Shall sum my count, and make my old excuse,'\n",
        "Proving his beauty by succession thine!\n",
        "This were to be new made when thou art old,\n",
        "And see thy blood warm when thou feel'st it cold.\"\"\".split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqGdI3ShTnH6",
        "outputId": "5a718f7b-d006-46f7-c5b5-e55ebd2bdcd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(['forty', 'When'], 'winters'), (['winters', 'forty'], 'shall'), (['shall', 'winters'], 'besiege')]\n"
          ]
        }
      ],
      "source": [
        "## N-Gram 데이터 생성\n",
        "# 원래는 입력을 토큰화해야하지만 해당 예제에서는 튜플 리스트를 생성\n",
        "# 각 튜플은 ([word_i-CONTEXT_SIZE, ..., word_i-1], 대상 단어) 형태\n",
        "\n",
        "ngrams = [\n",
        "    (\n",
        "        [test_sentence[i - j - 1] for j in range(CONTEXT_SIZE)], # 컨텍스트 단어들\n",
        "        test_sentence[i]\n",
        "    )\n",
        "    for i in range(CONTEXT_SIZE, len(test_sentence))\n",
        "]\n",
        "\n",
        "# 처음 3개의 데이터 확인하기\n",
        "print(ngrams[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dhn4juk-TnH7"
      },
      "outputs": [],
      "source": [
        "# 전체 텍스트에서 고유 단어를 추출하여 vocabulary map 생성\n",
        "vocab = set(test_sentence)\n",
        "word_to_ix = {word: i for i, word in enumerate(vocab)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MscdyOkTnH7"
      },
      "source": [
        "# **4. 모델링**\n",
        "- n-gram 언어 모델 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wJ65rSllTnH7"
      },
      "outputs": [],
      "source": [
        "## N-Gram 언어 모델 클래스 정의\n",
        "# 임베딩 레이어와 두 개의 선형 계층(Linear)으로 구성된 N-Gram 언어 모델\n",
        "class NGramLanguageModeler(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
        "        super(NGramLanguageModeler, self).__init__()\n",
        "        ## 임베딩 레이어 생성\n",
        "        # 주어진 \"어휘 크기\"와 \"임베딩 차원\"을 기반으로 랜덤하게 초기화된 임베딩 행렬 생성\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        ## 첫 번째 선형 계층\n",
        "        # input_dim: context_size * embedding_dim, output_dim: 128\n",
        "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
        "\n",
        "        ## 두 번째 선형 계층\n",
        "        # 첫 번째 선형 계층의 출력을 받아 어휘집 크기로 출력\n",
        "        self.linear2 = nn.Linear(128, vocab_size)\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # 임베딩 레이어를 통과하여 입력 벡터를 펼침(flatten)\n",
        "        embeds = self.embeddings(inputs).view((1, -1))\n",
        "        # 첫 번째 선형 계층 통과 및 활성화 함수 적용\n",
        "        out = F.relu(self.linear1(embeds))\n",
        "        # 두 번째 선형 계층 통과(활성화 함수 적용 x)\n",
        "        out = self.linear2(out)\n",
        "        # 출력층: 로그 소프트맥스를 통해 proportion 계산\n",
        "        log_probs = F.log_softmax(out, dim=1)\n",
        "\n",
        "        return log_probs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lh8BLxUlTnH7"
      },
      "source": [
        "# **5. 학습**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fRu24lPdTnH7"
      },
      "outputs": [],
      "source": [
        "## 손실 함수 및 최적화 함수(optimizer) 설정\n",
        "losses = []\n",
        "loss_function = nn.NLLLoss() # negative log-likelihood\n",
        "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE) # 모델 객체 생성\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001) # 경사 하강법 활용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PvoURbTTnH7",
        "outputId": "345cd919-239d-41ed-c160-afe362c351c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[519.2403349876404, 516.7696256637573, 514.3148913383484, 511.8742139339447, 509.44712257385254, 507.03318881988525, 504.6318883895874, 502.24181365966797, 499.86163806915283, 497.4908583164215]\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(10):\n",
        "    # 각 epoch에서의 total_loss 초기화\n",
        "    total_loss = 0\n",
        "\n",
        "    # 각 N-Gram 데이터를 활용하여..\n",
        "    for context, target in ngrams:\n",
        "        # vocabulary mapping(컨텍스트 단어 → 정수 인덱스)\n",
        "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
        "\n",
        "        ## forward pass\n",
        "        # 그래디언트 초기화\n",
        "        model.zero_grad()\n",
        "        # log probability 계산\n",
        "        log_probs = model(context_idxs)\n",
        "        # 손실 계산(대상 단어의 인덱스를 사용)\n",
        "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
        "\n",
        "        ## backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() # 현재 손실을 총 손실에 추가\n",
        "    # 각 에포크의 손실 저장\n",
        "    losses.append(total_loss)\n",
        "print(losses)  # 에포크 별 손실 출력 (훈련이 진행됨에 따라 감소)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDNpXTE9TnH7",
        "outputId": "b562ce18-d3b0-4f7e-cb1f-73567dbc4d13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.3439, -0.5519, -0.9614,  0.2754, -0.6782, -0.5463,  1.1518, -0.3991,\n",
            "         0.9745,  1.5451], grad_fn=<SelectBackward0>)\n"
          ]
        }
      ],
      "source": [
        "## 학습된 임베딩 확인\n",
        "# 특정 단어의 임베딩 결과 확인(예: \"beauty\")\n",
        "print(model.embeddings.weight[word_to_ix[\"beauty\"]])"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9TLzEGKWVDvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7jmy1XrKVtAa"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}